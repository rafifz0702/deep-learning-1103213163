# -*- coding: utf-8 -*-
"""dlweek4tensorflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKDeofHbFNS91Zzr7yuVDFLQ4KhdePCm
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Dropout
from tensorflow.keras import metrics
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve

# Load IMDB dataset with specific parameters
num_words = 50000  # Consider the top 50,000 words
maxlen = 300  # Maximum length of sequences
embedding_dim = 100  # Dimension of embedding vectors

print("Loading IMDB dataset...")
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

# Pad sequences to ensure equal length
print("Padding sequences...")
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)

"""# **Model RNN**"""

# Build RNN model with multiple layers and neurons
def build_rnn_model():
    model = Sequential()
    model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=maxlen))
    model.add(SimpleRNN(128, return_sequences=True))  # First RNN layer with 128 units
    model.add(Dropout(0.2))  # Dropout for regularization
    model.add(SimpleRNN(64))  # Second RNN layer with 64 units
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Initialize and train the model
model_rnn = build_rnn_model()

# Early stopping callback
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history_rnn = model_rnn.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stop])

# Evaluate the model
score_rnn = model_rnn.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {score_rnn[1]*100:.2f}%")

# Predict using the trained model
y_pred = model_rnn.predict(x_test)

# Convert predictions to binary values
y_pred_binary = (y_pred > 0.5).astype(int)

# Compute evaluation metrics
precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)
auc = roc_auc_score(y_test, y_pred)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Print evaluation metrics
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"AUC: {auc:.4f}")

# Plot ROC curve
plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Visualize accuracy and loss curves
plt.figure(figsize=(12, 6))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history_rnn.history['accuracy'], label='Training Accuracy')
plt.plot(history_rnn.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history_rnn.history['loss'], label='Training Loss')
plt.plot(history_rnn.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""# **Model LSTM**"""

# Build LSTM model with multiple layers and neurons
def build_lstm_model():
    model = Sequential()
    model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=maxlen))
    model.add(LSTM(128, return_sequences=True))  # First LSTM layer with 128 units
    model.add(Dropout(0.2))  # Dropout for regularization
    model.add(LSTM(64))  # Second LSTM layer with 64 units
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Initialize and train the model
model_lstm = build_lstm_model()

history_lstm = model_lstm.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stop])

# Evaluate the model
score_lstm = model_lstm.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy (LSTM): {score_lstm[1]*100:.2f}%")

# Predict and calculate metrics
y_pred_lstm = model_lstm.predict(x_test)
y_pred_lstm_binary = (y_pred_lstm > 0.5).astype(int)

precision_lstm = precision_score(y_test, y_pred_lstm_binary)
recall_lstm = recall_score(y_test, y_pred_lstm_binary)
f1_lstm = f1_score(y_test, y_pred_lstm_binary)
auc_lstm = roc_auc_score(y_test, y_pred_lstm)
fpr_lstm, tpr_lstm, thresholds_lstm = roc_curve(y_test, y_pred_lstm)

print(f"Precision (LSTM): {precision_lstm:.4f}")
print(f"Recall (LSTM): {recall_lstm:.4f}")
print(f"F1 Score (LSTM): {f1_lstm:.4f}")
print(f"AUC (LSTM): {auc_lstm:.4f}")

# LSTM ROC curve
plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM ROC curve (AUC = {auc_lstm:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve LSTM')
plt.legend(loc='lower right')
plt.show()

# Visualize accuracy and loss curves for LSTM
plt.figure(figsize=(12, 6))

# LSTM Accuracy
plt.subplot(1, 2, 1)
plt.plot(history_lstm.history['accuracy'], label='LSTM Training Accuracy')
plt.plot(history_lstm.history['val_accuracy'], label='LSTM Validation Accuracy')
plt.title('LSTM Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# LSTM Loss
plt.subplot(1, 2, 2)
plt.plot(history_lstm.history['loss'], label='LSTM Training Loss')
plt.plot(history_lstm.history['val_loss'], label='LSTM Validation Loss')
plt.title('LSTM Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Build GRU model with multiple layers and neurons
def build_gru_model():
    model = Sequential()
    model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=maxlen))
    model.add(GRU(128, return_sequences=True))  # First GRU layer with 128 units
    model.add(Dropout(0.2))  # Dropout for regularization
    model.add(GRU(64))  # Second GRU layer with 64 units
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Initialize and train the GRU model
model_gru = build_gru_model()

history_gru = model_gru.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stop])

# Evaluate the GRU model
score_gru = model_gru.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy (GRU): {score_gru[1]*100:.2f}%")

# Predict and calculate metrics
y_pred_gru = model_gru.predict(x_test)
y_pred_gru_binary = (y_pred_gru > 0.5).astype(int)

precision_gru = precision_score(y_test, y_pred_gru_binary)
recall_gru = recall_score(y_test, y_pred_gru_binary)
f1_gru = f1_score(y_test, y_pred_gru_binary)
auc_gru = roc_auc_score(y_test, y_pred_gru)
fpr_gru, tpr_gru, thresholds_gru = roc_curve(y_test, y_pred_gru)

print(f"Precision (GRU): {precision_gru:.4f}")
print(f"Recall (GRU): {recall_gru:.4f}")
print(f"F1 Score (GRU): {f1_gru:.4f}")
print(f"AUC (GRU): {auc_gru:.4f}")

# GRU ROC curve
plt.plot(fpr_gru, tpr_gru, label=f'GRU ROC curve (AUC = {auc_gru:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve GRU')
plt.legend(loc='lower right')
plt.show()

# Visualize accuracy and loss curves for GRU
plt.figure(figsize=(12, 6))

# GRU Accuracy
plt.subplot(1, 2, 1)
plt.plot(history_gru.history['accuracy'], label='GRU Training Accuracy')
plt.plot(history_gru.history['val_accuracy'], label='GRU Validation Accuracy')
plt.title('GRU Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# GRU Loss
plt.subplot(1, 2, 2)
plt.plot(history_gru.history['loss'], label='GRU Training Loss')
plt.plot(history_gru.history['val_loss'], label='GRU Validation Loss')
plt.title('GRU Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()