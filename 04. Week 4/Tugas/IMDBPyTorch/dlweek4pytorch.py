# -*- coding: utf-8 -*-
"""dlweek4pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fvOx2YBFutJj0sT8hFjiUJXoKwTASQXL
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Load IMDB dataset
num_words = 50000  # Consider the top 50,000 words
maxlen = 300  # Maximum length of sequences
embedding_dim = 100  # Dimension of embedding vectors

# Load the IMDB data
from tensorflow.keras.datasets import imdb
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

# Pad sequences to ensure equal length
from tensorflow.keras.preprocessing.sequence import pad_sequences
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)

# Convert to PyTorch tensors
x_train_tensor = torch.tensor(x_train, dtype=torch.long)
x_test_tensor = torch.tensor(x_test, dtype=torch.long)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Create DataLoader for training and test sets
train_data = TensorDataset(x_train_tensor, y_train_tensor)
test_data = TensorDataset(x_test_tensor, y_test_tensor)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64)

"""# **Model RNN**"""

# **Model RNN**

class RNNModel(nn.Module):
    def __init__(self, num_words, embedding_dim, maxlen):
        super(RNNModel, self).__init__()
        self.embedding = nn.Embedding(num_words, embedding_dim)
        self.rnn1 = nn.RNN(embedding_dim, 128, batch_first=True, dropout=0.2)
        self.rnn2 = nn.RNN(128, 64, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.rnn1(x)
        x, _ = self.rnn2(x)
        x = self.fc(x[:, -1, :])  # Get the output from the last time step
        return self.sigmoid(x)

# Initialize the model
rnn_model = RNNModel(num_words, embedding_dim, maxlen)

# Loss and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(rnn_model.parameters())

# Train the RNN model
def train_model(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    history = {'accuracy': [], 'loss': []}
    for epoch in range(epochs):
        total_loss = 0
        correct_preds = 0
        total_preds = 0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs.squeeze(), targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # Calculate accuracy
            preds = (outputs.squeeze() > 0.5).float()
            correct_preds += (preds == targets).sum().item()
            total_preds += len(targets)

        accuracy = correct_preds / total_preds * 100
        history['accuracy'].append(accuracy)
        history['loss'].append(total_loss / len(train_loader))
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%")

    return history

# Train the RNN model
history_rnn = train_model(rnn_model, train_loader, criterion, optimizer)

"""# **Model LTSM**"""

# **Model LSTM**

class LSTMModel(nn.Module):
    def __init__(self, num_words, embedding_dim, maxlen):
        super(LSTMModel, self).__init__()
        self.embedding = nn.Embedding(num_words, embedding_dim)
        self.lstm1 = nn.LSTM(embedding_dim, 128, batch_first=True, dropout=0.2)
        self.lstm2 = nn.LSTM(128, 64, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x = self.fc(x[:, -1, :])  # Get the output from the last time step
        return self.sigmoid(x)

# Initialize the model
lstm_model = LSTMModel(num_words, embedding_dim, maxlen)

# Loss and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(lstm_model.parameters())

# Train the LSTM model
history_lstm = train_model(lstm_model, train_loader, criterion, optimizer)

"""# **Model GRU**"""

# **Model GRU**

class GRUModel(nn.Module):
    def __init__(self, num_words, embedding_dim, maxlen):
        super(GRUModel, self).__init__()
        self.embedding = nn.Embedding(num_words, embedding_dim)
        self.gru1 = nn.GRU(embedding_dim, 128, batch_first=True, dropout=0.2)
        self.gru2 = nn.GRU(128, 64, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.gru1(x)
        x, _ = self.gru2(x)
        x = self.fc(x[:, -1, :])  # Get the output from the last time step
        return self.sigmoid(x)

# Initialize the model
gru_model = GRUModel(num_words, embedding_dim, maxlen)

# Loss and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(gru_model.parameters())

# Train the GRU model
history_gru = train_model(gru_model, train_loader, criterion, optimizer)

def evaluate_model(model, test_loader, criterion, model_name):
    model.eval()
    y_true = []
    y_pred = []
    total_loss = 0
    with torch.no_grad():
        for inputs, targets in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs.squeeze(), targets)
            total_loss += loss.item()

            preds = (outputs.squeeze() > 0.5).float()
            y_true.extend(targets.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    # Compute additional metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_pred)

    # Print metrics with a clear header for each model
    print(f"\n{model_name} Metrics:")
    print(f"Accuracy: {accuracy * 100:.2f}%")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")
    print(f"Loss: {total_loss / len(test_loader):.4f}")

    # Compute ROC Curve for visualization
    fpr, tpr, _ = roc_curve(y_true, y_pred)

    return accuracy, precision, recall, f1, auc, fpr, tpr, total_loss / len(test_loader)

# Call this function for each model (RNN, LSTM, GRU) to evaluate them
accuracy_rnn, precision_rnn, recall_rnn, f1_rnn, auc_rnn, fpr_rnn, tpr_rnn, loss_rnn = evaluate_model(rnn_model, test_loader, criterion, "RNN")
accuracy_lstm, precision_lstm, recall_lstm, f1_lstm, auc_lstm, fpr_lstm, tpr_lstm, loss_lstm = evaluate_model(lstm_model, test_loader, criterion, "LSTM")
accuracy_gru, precision_gru, recall_gru, f1_gru, auc_gru, fpr_gru, tpr_gru, loss_gru = evaluate_model(gru_model, test_loader, criterion, "GRU")

# **Plot ROC Curves**
plt.figure(figsize=(6, 6))

# RNN ROC curve
plt.plot(fpr_rnn, tpr_rnn, label=f'RNN ROC curve (AUC = {auc_rnn:.2f})')

# LSTM ROC curve
plt.plot(fpr_lstm, tpr_lstm, label=f'LSTM ROC curve (AUC = {auc_lstm:.2f})')

# GRU ROC curve
plt.plot(fpr_gru, tpr_gru, label=f'GRU ROC curve (AUC = {auc_gru:.2f})')

# Diagonal line for random classifier
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for RNN, LSTM, and GRU')
plt.legend(loc='lower right')
plt.show()