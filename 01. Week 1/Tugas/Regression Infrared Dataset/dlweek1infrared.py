# -*- coding: utf-8 -*-
"""DLWeek1Infrared.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oQUXNvkC8KQupREV3Ry1yfWqLwkemKwx
"""

import torch
import torch.nn as nn
import torch.optim as optim
import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/DLWeek1/Infrared.csv'
data = pd.read_csv(file_path)

data.head()

# Handle categorical variables
data['Gender'] = LabelEncoder().fit_transform(data['Gender'])
data['Ethnicity'] = LabelEncoder().fit_transform(data['Ethnicity'])
data['Age'] = LabelEncoder().fit_transform(data['Age'])

# Handle missing values (impute with mean)
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(data.drop(columns=['aveOralM']))  # Features
y = data['aveOralM']  # Target variable

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""# **PyTorch**"""

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Define the model (MLP - Multi-layer Perceptron)
class MLPModel(nn.Module):
    def __init__(self, input_size):
        super(MLPModel, self).__init__()
        self.layer1 = nn.Linear(input_size, 64)
        self.layer2 = nn.Linear(64, 32)
        self.layer3 = nn.Linear(32, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)
        return x

# Instantiate the model, loss function, and optimizer
model = MLPModel(input_size=X_train.shape[1])
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
epochs = 100
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()

    # Forward pass
    y_pred = model(X_train_tensor)

    # Compute loss
    loss = criterion(y_pred, y_train_tensor)

    # Backward pass and optimization
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Evaluate the model
model.eval()
y_test_pred = model(X_test_tensor)

# Calculate evaluation metrics
mse = torch.mean((y_test_pred - y_test_tensor)**2).item()  # MSE (as a float)
rmse = torch.sqrt(torch.tensor(mse)).item()  # RMSE (fix by converting mse to tensor first)
r2 = 1 - (torch.sum((y_test_tensor - y_test_pred)**2).item() / torch.sum((y_test_tensor - torch.mean(y_test_tensor)).pow(2)).item())

print(f'MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}')

"""## **TensorFlow**"""

# Build the model using TensorFlow/Keras
model_tf = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Compile the model
model_tf.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model_tf.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Predict and evaluate the model
y_test_pred = model_tf.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_test_pred)
rmse = mse ** 0.5
r2 = r2_score(y_test, y_test_pred)

print(f'MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}')