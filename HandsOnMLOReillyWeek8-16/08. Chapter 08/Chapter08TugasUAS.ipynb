{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using NumPy’s `svd()` function to obtain all the principal\n",
        "components of the training set, then extracts the two unit vectors that define the first\n",
        "two PCs."
      ],
      "metadata": {
        "id": "KHHtNEKCMk-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4duWQpAMgCh"
      },
      "outputs": [],
      "source": [
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "c1 = Vt.T[:, 0]\n",
        "c2 = Vt.T[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projecting the training set onto the plane defined by the first\n",
        "two principal components."
      ],
      "metadata": {
        "id": "1450-XKEMsj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = Vt.T[:, :2]\n",
        "X2D = X_centered.dot(W2)"
      ],
      "metadata": {
        "id": "WwSWIVGOMqlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying PCA to reduce the dimensionality\n",
        "of the dataset down to two dimensions."
      ],
      "metadata": {
        "id": "hSpnZuA2MwoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "X2D = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "ujSWqURLMwMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the explained variance ratios of the first two\n",
        "components of the 3D dataset."
      ],
      "metadata": {
        "id": "Q-uwsgyrM4so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "KR5ThGc-M3Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing PCA without reducing dimensionality, then computes\n",
        "the minimum number of dimensions required to preserve 95% of the training set’s\n",
        "variance."
      ],
      "metadata": {
        "id": "d9VTYgjPNAKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1"
      ],
      "metadata": {
        "id": "aTerW62XM-Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting `n_components` to be a float between 0.0 and 1.0, indicating the ratio\n",
        "of variance you wish to preserve instead of specifying the number of principal components you want to preserve."
      ],
      "metadata": {
        "id": "LF3yc3TbNJab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "Pde7VF1WNFlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compressing the MNIST dataset down to 154 dimensions, then\n",
        "uses the `inverse_transform()` method to decompress it back to 784 dimensions."
      ],
      "metadata": {
        "id": "fgkfMzMeNVjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 154)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "X_recovered = pca.inverse_transform(X_reduced)"
      ],
      "metadata": {
        "id": "NR9gu-4PNSQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the `svd_solver` hyperparameter to \"randomized\", Scikit-Learn uses a stochastic algorithm called Randomized PCA that quickly finds an approximation of the\n",
        "first d principal components."
      ],
      "metadata": {
        "id": "0RwDhC7CNcpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
        "X_reduced = rnd_pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "v_RJBsYSNcOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the MNIST dataset into 100 mini-batches (using NumPy’s\n",
        "`array_split()` function) and feeds them to Scikit-Learn’s IncrementalPCA class5\n",
        "to\n",
        "reduce the dimensionality of the MNIST dataset down to 154 dimensions."
      ],
      "metadata": {
        "id": "kChvzsqFNr_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(X_train, n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "\n",
        "X_reduced = inc_pca.transform(X_train)"
      ],
      "metadata": {
        "id": "FSNrDlCKNmPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using NumPy’s `memmap` class, which allows you to manipulate a\n",
        "large array stored in a binary file on disk as if it were entirely in memory; the class\n",
        "loads only the data it needs in memory, when it needs it."
      ],
      "metadata": {
        "id": "s_KfuoubN1av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
        "\n",
        "batch_size = m // n_batches\n",
        "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
        "inc_pca.fit(X_mm)"
      ],
      "metadata": {
        "id": "YgdBOosENzpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Scikit-Learn’s `KernelPCA` class to perform kPCA with an RBF\n",
        "kernel."
      ],
      "metadata": {
        "id": "9tIVguzcN_3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "rpmLtzBZN8jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a twostep pipeline, first reducing dimensionality to two dimensions using kPCA, then\n",
        "applying Logistic Regression for classification. Then it uses GridSearchCV to find the\n",
        "best kernel and gamma value for kPCA in order to get the best classification accuracy\n",
        "at the end of the pipeline."
      ],
      "metadata": {
        "id": "2YTApOHXOJrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline([\n",
        "        (\"kpca\", KernelPCA(n_components=2)),\n",
        "        (\"log_reg\", LogisticRegression())\n",
        "    ])\n",
        "\n",
        "param_grid = [{\n",
        "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
        "        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
        "    }]\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid_search.fit(X, y)"
      ],
      "metadata": {
        "id": "lva75LFKOFy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "Lq6VpQWBOUky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a\n",
        "supervised regression model, with the projected instances as the training set and the\n",
        "original instances as the targets that Scikit-Learn will do this automatically if you set\n",
        "`fit_inverse_transform=True`."
      ],
      "metadata": {
        "id": "3PUGzM_7OX3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
        " fit_inverse_transform=True)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "X_preimage = rbf_pca.inverse_transform(X_reduced)"
      ],
      "metadata": {
        "id": "jh4hMHVzOWNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the reconstruction pre-image error."
      ],
      "metadata": {
        "id": "VmV8l4Z2Oh-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.metrics import mean_squared_error\n",
        ">>> mean_squared_error(X, X_preimage)"
      ],
      "metadata": {
        "id": "HXSe57PpOgKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Scikit-Learn’s LocallyLinearEmbedding class to unroll the\n",
        "Swiss roll."
      ],
      "metadata": {
        "id": "VVB74_ZEOmob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_reduced = lle.fit_transform(X)"
      ],
      "metadata": {
        "id": "nglIAt5EOkNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}