{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create and train a voting classifier in Scikit-Learn, composed of\n",
        "three diverse classifiers. Training set is the moons dataset."
      ],
      "metadata": {
        "id": "ZcbJdfICHLd8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKlUO4ztHEdJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard')\n",
        "voting_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display each classifier’s accuracy on the test set."
      ],
      "metadata": {
        "id": "7uMQwUYFHanl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.metrics import accuracy_score\n",
        ">>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "...     clf.fit(X_train, y_train)\n",
        "...     y_pred = clf.predict(X_test)\n",
        "...     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "7fpFWMauHU7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train an\n",
        "ensemble of 500 Decision Tree classifiers. each is trained on 100 training instances\n",
        "randomly sampled from the training set with replacement. The `n_jobs`\n",
        "parameter tells Scikit-Learn the number of CPU cores to use for training and predictions."
      ],
      "metadata": {
        "id": "OyA1ZkxtHhwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "JRi_K0J_HfbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting `oob_score=True` when creating a `BaggingClassifier` to\n",
        "request an automatic oob evaluation after training. The resulting evaluation score is available through the `oob_score_` variable."
      ],
      "metadata": {
        "id": "_NeukBKnH_Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> bag_clf = BaggingClassifier(\n",
        "...     DecisionTreeClassifier(), n_estimators=500,\n",
        "...     bootstrap=True, n_jobs=-1, oob_score=True)\n",
        "...\n",
        ">>> bag_clf.fit(X_train, y_train)\n",
        ">>> bag_clf.oob_score_"
      ],
      "metadata": {
        "id": "grYYelhJH31F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.metrics import accuracy_score\n",
        ">>> y_pred = bag_clf.predict(X_test)\n",
        ">>> accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "J9F6AY-AIJkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> bag_clf.oob_decision_function_"
      ],
      "metadata": {
        "id": "CPQH1WCvIOlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use all\n",
        "available CPU cores to train a Random Forest classifier with 500 trees (each limited\n",
        "to maximum 16 nodes)."
      ],
      "metadata": {
        "id": "dJZXcJu3IanS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "Pb98b-v4IUNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training `BaggingClassifier` roughly equivalent to the previous `RandomForestClassifier`."
      ],
      "metadata": {
        "id": "Ao75yPMIIfwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
        "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
      ],
      "metadata": {
        "id": "Xozh9RwCIeEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a `RandomForestClassifier` on the iris dataset and\n",
        "outputs each feature’s importance."
      ],
      "metadata": {
        "id": "mNdyiSIII3Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.datasets import load_iris\n",
        ">>> iris = load_iris()\n",
        ">>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        ">>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        ">>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "...     print(name, score)"
      ],
      "metadata": {
        "id": "fIZ0d2p4IyPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train an AdaBoost classifier based on 200 Decision Stumps using\n",
        "Scikit-Learn’s `AdaBoostClassifier` class."
      ],
      "metadata": {
        "id": "RLYJN7i_JFM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "uzbU0T3XI_PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit a `DecisionTreeRegressor` to the training set (for example, a noisy quadratic training set)."
      ],
      "metadata": {
        "id": "WnhdurKEJlkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X, y)"
      ],
      "metadata": {
        "id": "1twTlQOxJjE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a second DecisionTreeRegressor on the residual errors made by the\n",
        "first predictor."
      ],
      "metadata": {
        "id": "KFRVzZajJyc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X, y2)"
      ],
      "metadata": {
        "id": "kN4mwaNlJwj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a third regressor on the residual errors made by the second predictor."
      ],
      "metadata": {
        "id": "DbTe00_ZJ8LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X, y3)"
      ],
      "metadata": {
        "id": "WhlJYSJ2J1LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make predictions on a new\n",
        "instance simply by adding up the predictions of all the trees using the ensemble containing three trees."
      ],
      "metadata": {
        "id": "IE3Ph7a6KCCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ],
      "metadata": {
        "id": "f98E6xB7KA4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Scikit-Learn’s `GradientBoostingRegressor` class to create the same ensemble as the previous one with hyperparameters to\n",
        "control the growth of Decision Trees, as well as\n",
        "hyperparameters to control the ensemble training."
      ],
      "metadata": {
        "id": "O7esJGYBKhn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt.fit(X, y)"
      ],
      "metadata": {
        "id": "KIiVYtioKgeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a GBRT ensemble with\n",
        "120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal\n",
        "number of trees."
      ],
      "metadata": {
        "id": "fe310ryjLIbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "            for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ODFGGgn6K5VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopping training when the validation error does not\n",
        "improve for five iterations in a row. Setting `warm_start=True`, which makes ScikitLearn keep existing trees when the `fit()` method is called, allowing incremental\n",
        "training."
      ],
      "metadata": {
        "id": "5M9ewbg7LQrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    y_pred = gbrt.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break # early stopping"
      ],
      "metadata": {
        "id": "HVlHeZdYLPxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using an optimized implementation of Gradient Boosting, XGBoost's API, that aims to be extremely fast, scalable,\n",
        "and portable."
      ],
      "metadata": {
        "id": "NGnuNrgrLzjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "id": "BmJBjx1sLpcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A feature of XGBoost that automatically takes care of early\n",
        "stopping."
      ],
      "metadata": {
        "id": "W5HzzyUqMD1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_reg.fit(X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "id": "4_mqoqH2MAKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}